{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Neural Networks\n",
    "In the previous exercise, you implemented feedforward propagation for neu-\n",
    "ral networks and used it to predict handwritten digits with the weights we\n",
    "provided. In this exercise, you will implement the backpropagation algorithm\n",
    "to learn the parameters for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setup the parameters you will use for this exercise\n",
    "input_layer_size=400\n",
    "hidden_layer_size=25\n",
    "num_labels=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Training Data\n",
    "dataFile='ex4data1.mat'\n",
    "data=scio.loadmat(dataFile)\n",
    "X=data['X']\n",
    "y=data['y']\n",
    "m=X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Visualizing the data\n",
    "randomly select 100 data points to display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Model representation\n",
    "Our neural network has 3 layers – an input layer,\n",
    "a hidden layer and an output layer. Recall that our inputs are pixel values\n",
    "3of digit images. Since the images are of size 20 × 20, this gives us 400 input\n",
    "layer units (not counting the extra bias unit which always outputs +1).\n",
    "\n",
    "You have been provided with a set of network parameters (Θ (1) , Θ (2) )\n",
    "already trained by us. These are stored in ex4weights.mat and will be\n",
    "loaded by ex4.m into Theta1 and Theta2. The parameters have dimensions\n",
    "that are sized for a neural network with 25 units in the second layer and 10\n",
    "output units (corresponding to the 10 digit classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the weights into variables Theta1 and Theta2\n",
    "dataFile='ex4weights.mat'\n",
    "data=scio.loadmat(dataFile)\n",
    "Theta1=data['Theta1']\n",
    "Theta2=data['Theta2']\n",
    "#Unroll parameters\n",
    "nn_params=np.hstack((Theta1.ravel(),Theta2.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Feedforward and cost function\n",
    "Now you will implement the cost function and gradient for the neural net-\n",
    "work. First, complete the code in nnCostFunction.m to return the cost.\n",
    "4Recall that the cost function for the neural network (without regulariza-\n",
    "tion) is\n",
    "$$J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}[-y_{k}^{(i)}log((h_{\\theta}(x^{(i)}))_{k})-(1-y_{k}^{i})log(1-(h_{\\theta}(x^{(i)}))_{k})]$$\n",
    "where h θ (x (i) ) is computed as shown in the Figure 2 and K = 10 is the total\n",
    "(3)\n",
    "number of possible labels. Note that h θ (x (i) ) k = a k is the activation (output\n",
    "value) of the k-th output unit. Also, recall that whereas the original labels\n",
    "(in the variable y) were 1, 2, ..., 10, for the purpose of training a neural\n",
    "network, we need to recode the labels as vectors containing only values 0 or\n",
    "1, so that\n",
    "$$y=\\begin{bmatrix}1\\\\0 \\\\ 0\\\\ \\vdots \\\\ 0\\end{bmatrix},\\begin{bmatrix}0\\\\1 \\\\ 0\\\\ \\vdots \\\\ 0\\end{bmatrix},\\cdots or\\begin{bmatrix}0\\\\ 0\\\\ 0\\\\\\vdots\\\\1 \\end{bmatrix}$$\n",
    "For example, if x (i) is an image of the digit 5, then the corresponding\n",
    "y (i) (that you should use with the cost function) should be a 10-dimensional\n",
    "vector with y 5 = 1, and the other elements equal to 0.\n",
    "You should implement the feedforward computation that computes h θ (x (i) )\n",
    "for every example i and sum the cost over all examples. Your code should\n",
    "also work for a dataset of any size, with any number of labels (you\n",
    "can assume that there are always at least K ≥ 3 labels).\n",
    "\n",
    "Implementation Note: The matrix X contains the examples in rows\n",
    "(i.e., X(i,:)’ is the i-th training example x (i) , expressed as a n × 1\n",
    "vector.) When you complete the code in nnCostFunction.m, you will\n",
    "need to add the column of 1’s to the X matrix. The parameters for each\n",
    "unit in the neural network is represented in Theta1 and Theta2 as one\n",
    "row. Specifically, the first row of Theta1 corresponds to the first hidden\n",
    "unit in the second layer. You can use a for-loop over the examples to\n",
    "compute the cost.\n",
    "\n",
    "Once you are done, ex4.m will call your nnCostFunction using the loaded\n",
    "set of parameters for Theta1 and Theta2. You should see that the cost is\n",
    "about 0.287629."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define some useful function firstly\n",
    "def sigmoid(z):\n",
    "    res=1/(1+np.exp(-z))\n",
    "    return res\n",
    "def h(x):\n",
    "    res=sigmoid(x)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#================ Part 3: Compute Cost (Feedforward) ================\n",
    "#  To the neural network, you should first start by implementing the\n",
    "#  feedforward part of the neural network that returns the cost only. After\n",
    "#  implementing the feedforward to compute the cost, you can verify that\n",
    "#  your implementation is correct by verifying that you get the same cost\n",
    "#  as us for the fixed debugging parameters.\n",
    "#\n",
    "#  We suggest implementing the feedforward cost *without* regularization\n",
    "#  first so that it will be easier for you to debug. Later, in part 4, you\n",
    "#  will get to implement the regularized cost.\n",
    "\n",
    "def nnCostFunction(nn_params,input_layer_size,hidden_layer_size,\\\n",
    "                  num_labels,X,y,lambd):\n",
    "    # NNCOSTFUNCTION Implements the neural network cost function for a \n",
    "    # two layer neural network which performs classification\n",
    "    # [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels,\n",
    "    # X, y, lambda) computes the cost and gradient of the neural network. \n",
    "    # The parameters for the neural network are \"unrolled\" into the vector\n",
    "    # nn_params and need to be converted back into the weight matrices. \n",
    "    \n",
    "    # The returned parameter grad should be a \"unrolled\" vector of the\n",
    "    # partial derivatives of the neural network.\n",
    "\n",
    "\n",
    "\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, \n",
    "    # the weight matrices\n",
    "    # for our 2 layer neural network\n",
    "    Theta1,Theta2=np.hsplit(nn_params,((input_layer_size+1)*\\\n",
    "                                       hidden_layer_size,))\n",
    "    Theta1=Theta1.reshape((hidden_layer_size,(input_layer_size+1)))\n",
    "    Theta2=Theta2.reshape((num_labels,(hidden_layer_size+1)))\n",
    "    \n",
    "    # Setup some useful variables\n",
    "    m=X.shape[0]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    J=0\n",
    "    Theta1_grad=np.zeros(Theta1.shape)\n",
    "    Theta2_grad=np.zeros(Theta2.shape)\n",
    "    \n",
    "#Part 1: Feedforward the neural network and return the cost in the \n",
    "#variable J. \n",
    "    #change y(i) to vector\n",
    "    y_new=np.zeros((m,num_labels))\n",
    "    for i in range(m):\n",
    "        for j in range(1,num_labels+1):\n",
    "            if y[i][0]==j:\n",
    "                y_new[i][j-1]=1\n",
    "    #add a column of 1 to X\n",
    "    X=np.hstack((np.ones((m,1)),X))\n",
    "    hidden=h(np.dot(X,Theta1.T)) #compute the hidden layer\n",
    "    hidden=np.hstack((np.ones((m,1)),hidden))\n",
    "    output=h(np.dot(hidden,Theta2.T))\n",
    "    #choose the theta with penalize\n",
    "    Theta1_penalize=np.hsplit(Theta1,(1,))[1]\n",
    "    Theta2_penalize=np.hsplit(Theta2,(1,))[1]\n",
    "    J=np.sum(-y_new*np.log(output)-(1-y_new)*np.log(1-output))/m+\\\n",
    "    (lambd/(2*m))*(np.sum(Theta1_penalize**2)+np.sum(Theta2_penalize**2))\n",
    "\n",
    "#Part 2: Implement the backpropagation algorithm to compute the gradients\n",
    "# Theta1_grad and Theta2_grad. You should return the partial derivatives of\n",
    "# the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n",
    "# Theta2_grad, respectively. After implementing Part 2, you can check\n",
    "# that your implementation is correct by running checkNNGradients\n",
    "\n",
    "# Note: The vector y passed into the function is a vector of labels\n",
    "#       containing values from 1..K. You need to map this vector into a \n",
    "#       binary vector of 1's and 0's to be used with the neural network\n",
    "#       cost function.\n",
    "\n",
    "# Hint: We recommend implementing backpropagation using a for-loop\n",
    "#       over the training examples if you are implementing it for the \n",
    "#       first time.\n",
    "    #set Delta2 and Delta3\n",
    "    Delta2=np.zeros(Theta2.shape)\n",
    "    Delta1=np.zeros(Theta1.shape)\n",
    "    for i in range(m):\n",
    "        #set a(1)=x(i)\n",
    "        a1=X[i,:].reshape((input_layer_size+1,1))\n",
    "        #perform foward propagation to compute a(l) for l=1,2,3.....L\n",
    "        a2=h(np.dot(Theta1,a1))\n",
    "        a2=np.vstack((np.ones((1,1)),a2))\n",
    "        a3=h(np.dot(Theta2,a2))\n",
    "        #Using delta(L)=a(L)-y(i)\n",
    "        delta3=a3-y_new[i].reshape((num_labels,1))\n",
    "        #perform back propagation to compute all previous layer error vector\n",
    "        delta2=np.dot(Theta2.T,delta3)*a2*(1-a2)\n",
    "        delta2=np.vsplit(delta2,(1,))[1] #需要把bias unit(delta2_0)丢弃\n",
    "        Delta1=Delta1+np.dot(delta2,a1.T)\n",
    "        Delta2=Delta2+np.dot(delta3,a2.T)\n",
    "    #computer the derivative matrix D2 and D1\n",
    "    Theta2_grad=Delta2/m+(lambd/m)*Theta2\n",
    "    Theta2_grad[:,0]-=(lambd/m)*Theta2[:,0]\n",
    "    Theta1_grad=Delta1/m+(lambd/m)*Theta1\n",
    "    Theta1_grad[:,0]-=(lambd/m)*Theta1[:,0]\n",
    "    #Unroll gradients\n",
    "    grad=np.hstack((Theta1_grad.ravel(),Theta2_grad.ravel()))\n",
    "    return J,grad\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters(loaded from ex4weights):\n",
      "This value should be about 0.287629\n",
      "My value is 0.287629165161\n"
     ]
    }
   ],
   "source": [
    "#test my nnCostFunction\n",
    "# Weight regularization parameter (we set this to 0 here).\n",
    "lambd=0\n",
    "J,grad=nnCostFunction(nn_params,input_layer_size,hidden_layer_size,\\\n",
    "                      num_labels,X,y,lambd)\n",
    "print('Cost at parameters(loaded from ex4weights):')\n",
    "print('This value should be about 0.287629')\n",
    "print('My value is',J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters(loaded from ex4weights):\n",
      "This value should be about 0.383770\n",
      "My value is 0.383769859091\n"
     ]
    }
   ],
   "source": [
    "# =============== Part 4: Implement Regularization ===============\n",
    "# Once your cost function implementation is correct, you should now\n",
    "# continue to implement the regularization with the cost.\n",
    "\n",
    "# Weight regularization parameter (we set this to 1 here)\n",
    "lambd=1\n",
    "J,grad=nnCostFunction(nn_params,input_layer_size,hidden_layer_size,\\\n",
    "                      num_labels,X,y,lambd)\n",
    "print('Cost at parameters(loaded from ex4weights):')\n",
    "print('This value should be about 0.383770')\n",
    "print('My value is',J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Backprpagation\n",
    "In this part of the exercise, you will implement the backpropagation algo-\n",
    "rithm to compute the gradient for the neural network cost function. You\n",
    "will need to complete the nnCostFunction.m so that it returns an appropri-\n",
    "ate value for grad. Once you have computed the gradient, you will be able\n",
    "to train the neural network by minimizing the cost function J(Θ) using an\n",
    "advanced optimizer such as fmincg.\n",
    "You will first implement the backpropagation algorithm to compute the\n",
    "gradients for the parameters for the (unregularized) neural network. After you have verified that your gradient computation for the unregularized case\n",
    "is correct, you will implement the gradient for the regularized neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Sigmoid gradient\n",
    "To help you get started with this part of the exercise, you will first implement\n",
    "the sigmoid gradient function. The gradient for the sigmoid function can be\n",
    "computed as\n",
    "$${g}'(z)=\\frac{d}{dz}g(z)=g(z)(1-g(z))$$\n",
    "where\n",
    "$$sigmoid(z)=g(z)=\\frac{1}{1+e^{-z}}$$\n",
    "When you are done, try testing a few values by calling sigmoidGradient(z)\n",
    "at the Octave/MATLAB command line. For large values (both positive and\n",
    "negative) of z, the gradient should be close to 0. When z = 0, the gradi-\n",
    "ent should be exactly 0.25. Your code should also work with vectors and\n",
    "matrices. For a matrix, your function should perform the sigmoid gradient\n",
    "function on every element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19661193  0.23500371  0.25        0.23500371  0.19661193]\n",
      "My sigmoidGradient function when z=0: 0.25 itshould be 0.25\n"
     ]
    }
   ],
   "source": [
    "# ================ Part 5: Sigmoid Gradient  ================\n",
    "# Before you start implementing the neural network, you will first\n",
    "# implement the gradient for the sigmoid function.\n",
    "def sigmoidGradient(z):\n",
    "    #returns the gradient of the sigmoid function\n",
    "    g=sigmoid(z)*(1-sigmoid(z))\n",
    "    return g\n",
    "#test sigmoidGradient function\n",
    "g=sigmoidGradient(np.array([-1,0.5,0,0.5,1]))\n",
    "print(g)\n",
    "print('My sigmoidGradient function when z=0:',sigmoidGradient(0),'it\\\n",
    "should be 0.25')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Random initialization\n",
    "When training neural networks, it is important to randomly initialize the pa-\n",
    "rameters for symmetry breaking. One effective strategy for random initializa-\n",
    "tion is to randomly select values for Θ (l) uniformly in the range [−\u000f init , \u000f init ].\n",
    "You should use \u000f init = 0.12. 2 This range of values ensures that the parameters\n",
    "are kept small and makes the learning more efficient.\n",
    "Your job is to complete randInitializeWeights.m to initialize the weights\n",
    "for Θ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ================ Part 6: Initializing Pameters ================\n",
    "# In this part of the exercise, you will be starting to implment a two\n",
    "# layer neural network that classifies digits. You will start by\n",
    "# implementing a function to initialize the weights of the neural network\n",
    "# (randInitializeWeights.m)\n",
    "def randInitializeWeights(L_in,L_out):\n",
    "    epsilon_init=0.12\n",
    "    W=np.random.random((L_out,1+L_in))*2*epsilon_init-epsilon_init\n",
    "    return W\n",
    "#initialize \n",
    "initial_Theta1=randInitializeWeights(input_layer_size,hidden_layer_size)\n",
    "initial_Theta2=randInitializeWeights(hidden_layer_size,num_labels)\n",
    "# Unroll parameters\n",
    "initial_nn_params=np.hstack((initial_Theta1.ravel(),initial_Theta2.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Backpropagation\n",
    "Now, you will implement the backpropagation algorithm. Recall that\n",
    "the intuition behind the backpropagation algorithm is as follows. Given a\n",
    "training example (x (t) , y (t) ), we will first run a “forward pass” to compute\n",
    "all the activations throughout the network, including the output value of the\n",
    "hypothesis h Θ (x). Then, for each node j in layer l, we would like to compute\n",
    "(l)\n",
    "an “error term” δ j that measures how much that node was “responsible”\n",
    "for any errors in our output.\n",
    "\n",
    "For an output node, we can directly measure the difference between the\n",
    "(3)\n",
    "network’s activation and the true target value, and use that to define δ j\n",
    "(since layer 3 is the output layer). For the hidden units, you will compute\n",
    "(l)\n",
    "δ j based on a weighted average of the error terms of the nodes in layer\n",
    "(l + 1).\n",
    "\n",
    "In detail, here is the backpropagation algorithm (also depicted in Figure\n",
    "3). You should implement steps 1 to 4 in a loop that processes one example\n",
    "at a time. Concretely, you should implement a for-loop for t = 1:m and\n",
    "place steps 1-4 below inside the for-loop, with the t th iteration performing\n",
    "the calculation on the t th training example (x (t) , y (t) ). Step 5 will divide the\n",
    "accumulated gradients by m to obtain the gradients for the neural network\n",
    "cost function.\n",
    "\n",
    "1.Set the input layer’s values (a (1) ) to the t-th training example x (t) .\n",
    "Perform a feedforward pass (Figure 2), computing the activations (z (2) , a (2) , z (3) , a (3) )\n",
    "for layers 2 and 3. Note that you need to add a +1 term to ensure that\n",
    "the vectors of activations for layers a (1) and a (2) also include the bias\n",
    "unit. In Octave/MATLAB, if a 1 is a column vector, adding one corre-\n",
    "sponds to a 1 = [1 ; a 1].\n",
    "\n",
    "2.For each output unit k in layer 3 (the output layer), set\n",
    "$$\\delta _{k}^{(3)}=(a_{k}^{(3)}-y_{k})$$\n",
    "where y k ∈ {0, 1} indicates whether the current training example be-\n",
    "longs to class k (y k = 1), or if it belongs to a different class (y k = 0).\n",
    "You may find logical arrays helpful for this task (explained in the pre-\n",
    "vious programming exercise).\n",
    "\n",
    "3.For the hidden layer l = 2, set\n",
    "$$\\delta ^{(2)}=(\\Theta ^{(2)})^{T}\\delta^{3}.*{g}'(z^{(2)})$$\n",
    "\n",
    "4.Accumulate the gradient from this example using the following formula. Note that you should skip or remove $\\delta_{0}^{(2)}$.\n",
    "$$\\Delta^{(l)}=\\Delta^{(l)}+\\delta^{(l+1)}(a^{(l)})^{T}$$\n",
    "\n",
    "5.Obtain the (unregularized) gradient for the neural network cost func-\n",
    "tion by dividing the accumulated gradients by $\\frac{1}{m}$ :\n",
    "$$\\frac{\\partial }{\\partial \\Theta _{ij}^{(l)}}J(\\Theta)=D_{ij}^{(l)}=\\frac{1}{m}\\Delta_{ij}^{(l)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.03826066e-02   5.83963249e-04  -1.60611136e-04  -5.02074537e-04\n",
      "   4.80491536e-03   8.43671620e-05   7.82443731e-04   1.01369272e-03\n",
      "  -5.42498551e-03  -5.06064287e-04   8.41872849e-04  -5.03726757e-05\n",
      "   1.29444034e-03  -5.84469268e-05   4.42557660e-04  -5.84027351e-04\n",
      "  -7.48285858e-03  -3.05010375e-04  -4.43838804e-04  -6.45498529e-04\n",
      "   3.06304371e-01   1.49995407e-01   1.57387635e-01   1.55097923e-01\n",
      "   1.54284844e-01   1.48149358e-01   1.47421283e-01   7.20584647e-02\n",
      "   7.64431428e-02   7.44203972e-02   7.34620195e-02   7.21468055e-02\n",
      "   1.19201379e-01   5.81496899e-02   6.13301183e-02   6.12443456e-02\n",
      "   6.10513925e-02   5.72022472e-02]\n",
      "[  1.03826066e-02   5.83963247e-04  -1.60611138e-04  -5.02074535e-04\n",
      "   4.80491536e-03   8.43671599e-05   7.82443730e-04   1.01369272e-03\n",
      "  -5.42498551e-03  -5.06064284e-04   8.41872847e-04  -5.03726749e-05\n",
      "   1.29444033e-03  -5.84469295e-05   4.42557664e-04  -5.84027353e-04\n",
      "  -7.48285857e-03  -3.05010377e-04  -4.43838803e-04  -6.45498528e-04\n",
      "   3.06304371e-01   1.49995407e-01   1.57387635e-01   1.55097923e-01\n",
      "   1.54284844e-01   1.48149358e-01   1.47421283e-01   7.20584647e-02\n",
      "   7.64431428e-02   7.44203972e-02   7.34620195e-02   7.21468055e-02\n",
      "   1.19201379e-01   5.81496899e-02   6.13301183e-02   6.12443456e-02\n",
      "   6.10513925e-02   5.72022472e-02]\n",
      "If your backpropagation implementation is correct,then the relavive    difference will be small(less than 1e-9)\n",
      "Relative Difference: 4.25974076318e-11\n",
      "[ 0.00102293 -0.00915989  0.06583414 -0.00686629 -0.00148213  0.00412358\n",
      "  0.00761689 -0.02083902 -0.00504239 -0.03716796 -0.01189488 -0.06810485\n",
      " -0.00955803  0.0230673   0.04128753  0.0140419  -0.00142378 -0.07041173\n",
      "  0.05479796  0.00824319  0.26438902  0.12720576  0.13261883  0.08233558\n",
      "  0.06778852  0.14010822  0.07671551  0.01915829  0.00865726 -0.004801\n",
      " -0.014262   -0.00069065  0.10958973  0.12504464  0.05599491  0.08193503\n",
      "  0.03021512  0.02753497]\n",
      "[ 0.00102293 -0.00915989  0.06583414 -0.00686629 -0.00148213  0.00412358\n",
      "  0.00761689 -0.02083902 -0.00504239 -0.03716796 -0.01189488 -0.06810485\n",
      " -0.00955803  0.0230673   0.04128753  0.0140419  -0.00142378 -0.07041173\n",
      "  0.05479796  0.00824319  0.26438902  0.12720576  0.13261883  0.08233558\n",
      "  0.06778852  0.14010822  0.07671551  0.01915829  0.00865726 -0.004801\n",
      " -0.014262   -0.00069065  0.10958973  0.12504464  0.05599491  0.08193503\n",
      "  0.03021512  0.02753497]\n",
      "If your backpropagation implementation is correct,then the relavive    difference will be small(less than 1e-9)\n",
      "Relative Difference: 4.48355017811e-11\n",
      "Cost at (fixed) debugging parameters with lambda=3 should be about0.576051.My value is 0.57605124695\n"
     ]
    }
   ],
   "source": [
    "# =============== Part 7: Implement Backpropagation ===============\n",
    "# Once your cost matches up with ours, you should proceed to implement the\n",
    "# backpropagation algorithm for the neural network. You should add to the\n",
    "# code you've written in nnCostFunction.m to return the partial\n",
    "# derivatives of the parameters.\n",
    "\n",
    "def computeNumericalGradient1(theta,input_layer_size,hidden_layer_size,\\\n",
    "                            num_labels,X,y,lambd):\n",
    "    numgrad=np.zeros(theta.shape)\n",
    "    print(theta.shape)\n",
    "    e=10**(-4)\n",
    "    for i in range(theta.shape[0]):\n",
    "        #set perturbation vector\n",
    "        thetaPlus=theta\n",
    "        thetaPlus[i]=thetaPlus[i]+e\n",
    "        thetaMinus=theta\n",
    "        thetaMinus[i]=thetaMinus[i]-e\n",
    "        numgrad[i]=(nnCostFunction(thetaPlus,input_layer_size,\\\n",
    "                                  hidden_layer_size,num_labels,X,y,lambd)[0]\\\n",
    "                   -nnCostFunction(thetaMinus,input_layer_size,\\\n",
    "                                  hidden_layer_size,num_labels,X,y,lambd)[0])\\\n",
    "        /(2*e)\n",
    "        return numgrad\n",
    "def computeNumericalGradient(theta,input_layer_size,hidden_layer_size,\\\n",
    "                            num_labels,X,y,lambd):\n",
    "    numgrad=np.zeros(theta.shape)\n",
    "    perturb=np.zeros(theta.shape)\n",
    "    e=10**(-4)\n",
    "    for i in range(theta.shape[0]):\n",
    "        perturb[i]=e\n",
    "        loss1=nnCostFunction(theta-perturb,input_layer_size,\\\n",
    "                                  hidden_layer_size,num_labels,X,y,lambd)[0]\n",
    "        loss2=nnCostFunction(theta+perturb,input_layer_size,\\\n",
    "                                  hidden_layer_size,num_labels,X,y,lambd)[0]\n",
    "        numgrad[i]=(loss2-loss1)/(2*e)\n",
    "        perturb[i]=0\n",
    "    return numgrad\n",
    "\n",
    "def checkNNGradients(lambd):\n",
    "    #CHECKNNGRADIENTS Creates a small neural network to check the\n",
    "    #backpropagation gradients\n",
    "    #CHECKNNGRADIENTS(lambda) Creates a small neural network to check the\n",
    "    #backpropagation gradients, it will output the analytical gradients\n",
    "    #produced by your backprop code and the numerical gradients (computed\n",
    "    #using computeNumericalGradient). These two gradient computations should\n",
    "    #result in very similar values.\n",
    "    input_layer_size=3\n",
    "    hidden_layer_size=5\n",
    "    num_labels=3\n",
    "    m=5\n",
    "    #we generate some 'random' test data\n",
    "    Theta1=randInitializeWeights(input_layer_size,hidden_layer_size)\n",
    "    Theta2=randInitializeWeights(hidden_layer_size,num_labels)\n",
    "    X=randInitializeWeights(input_layer_size-1,m)\n",
    "    y=np.array([[2],[3],[1],[2],[3]])\n",
    "    \n",
    "    #Unroll parameters\n",
    "    nn_params=np.hstack((Theta1.ravel(),Theta2.ravel()))\n",
    "    \n",
    "    cost,grad=nnCostFunction(nn_params,input_layer_size,hidden_layer_size,\\\n",
    "                            num_labels,X,y,lambd)\n",
    "    numgrad=computeNumericalGradient(nn_params,input_layer_size,\\\n",
    "                                     hidden_layer_size,num_labels,X,y,lambd)\n",
    "    #np.linalg.norm计算矩阵的二范数\n",
    "    diff=np.linalg.norm(numgrad-grad,ord=2)/np.linalg.norm(numgrad+grad,ord=2)\n",
    "    print(grad)\n",
    "    print(numgrad)\n",
    "    print('If your backpropagation implementation is correct,then the relavive\\\n",
    "    difference will be small(less than 1e-9)')\n",
    "    print('Relative Difference:',diff)\n",
    "\n",
    "# Check gradients by running checkNNGradients\n",
    "lambd=0\n",
    "checkNNGradients(lambd)\n",
    "lambd=3\n",
    "checkNNGradients(lambd)\n",
    "debug_J=nnCostFunction(nn_params, input_layer_size,hidden_layer_size, num_labels, X, y, lambd)[0]\n",
    "print('Cost at (fixed) debugging parameters with lambda=3 should be about\\\n",
    "0.576051.My value is',debug_J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghaoxu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:56: RuntimeWarning: divide by zero encountered in log\n",
      "/home/zhanghaoxu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:56: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 0.029401584529723267\n",
      "     jac: array([ -3.36726589e-04,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         8.56081253e-05,  -8.58037625e-05,  -8.80275378e-05])\n",
      " message: 'Linear search failed'\n",
      "    nfev: 343\n",
      "     nit: 21\n",
      "  status: 4\n",
      " success: False\n",
      "       x: array([-1.35257101,  0.06775727, -0.10249001, ..., -2.21698907,\n",
      "        5.90792953,  4.29772302])\n"
     ]
    }
   ],
   "source": [
    "# =================== Part 8: Training NN ===================\n",
    "# You have now implemented all the code necessary to train a neural \n",
    "# network. To train your neural network, I will use \"minimize\"\n",
    "lambd=0\n",
    "optimizeResult=minimize(fun=nnCostFunction,x0=initial_nn_params,jac=True,method='TNC',\\\n",
    "                       args=(input_layer_size,hidden_layer_size,num_labels,X,y,lambd),\\\n",
    "                       options={'maxiter':800})\n",
    "print(optimizeResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.92%\n"
     ]
    }
   ],
   "source": [
    "def feedpropagation(theta,X):\n",
    "    m=X.shape[0]\n",
    "    X=np.hstack((np.ones((X.shape[0],1)),X))\n",
    "    Theta1,Theta2=np.hsplit(theta,(401*25,))\n",
    "    Theta1=Theta1.reshape(25,401)\n",
    "    Theta2=Theta2.reshape(10,26)\n",
    "    A2=h(np.dot(X,Theta1.T))\n",
    "    A2=np.hstack((np.ones((m,1)),A2))\n",
    "    A3=h(np.dot(A2,Theta2.T))\n",
    "    return A3\n",
    "def computeAccuracy(y,A3):\n",
    "    m=A3.shape[0]\n",
    "    y_predict=np.zeros((m,))\n",
    "    for i in range(m):\n",
    "        y_predict[i]=np.argmax(A3[i])+1\n",
    "    num=0\n",
    "    for i in range(m):\n",
    "        if y[i][0]==y_predict[i]:num+=1\n",
    "    accuracy=num/m\n",
    "    return str(accuracy*100)+'%'\n",
    "A3=feedpropagation(optimizeResult.x,X)\n",
    "accuracy=computeAccuracy(y,A3)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd=3\n",
    "optimizeResult=minimize(fun=nnCostFunction,x0=initial_nn_params,jac=True,method='TNC',\\\n",
    "                       args=(input_layer_size,hidden_layer_size,num_labels,X,y,lambd),\\\n",
    "                       options={'maxiter':800})\n",
    "print(optimizeResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambd=1\n",
    "optimizeResult=minimize(fun=nnCostFunction,x0=initial_nn_params,jac=True,method='TNC',\\\n",
    "                       args=(input_layer_size,hidden_layer_size,num_labels,X,y,lambd),\\\n",
    "                       options={'maxiter':800})\n",
    "print(optimizeResult)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
