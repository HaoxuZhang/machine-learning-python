{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Spam Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "from sklearn.svm import SVC\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer #词干提取算法\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocessing Emails\n",
    "• Lower-casing:\n",
    "The entire email is converted into lower case, so\n",
    "that captialization is ignored (e.g., IndIcaTE is treated the same as\n",
    "Indicate).\n",
    "\n",
    "• Stripping HTML: All HTML tags are removed from the emails.\n",
    "Many emails often come with HTML formatting; we remove all the\n",
    "HTML tags, so that only the content remains.\n",
    "\n",
    "• Normalizing URLs: All URLs are replaced with the text “httpaddr”.\n",
    "\n",
    "• Normalizing Email Addresses:\n",
    "with the text “emailaddr”.\n",
    "\n",
    "• Normalizing Numbers:\n",
    "“number”.\n",
    "All email addresses are replaced\n",
    "All numbers are replaced with the text\n",
    "\n",
    "• Normalizing Dollars: All dollar signs ($) are replaced with the text\n",
    "“dollar”.\n",
    "\n",
    "• Word Stemming: Words are reduced to their stemmed form. For ex-\n",
    "ample, “discount”, “discounts”, “discounted” and “discounting” are all\n",
    "replaced with “discount”. Sometimes, the Stemmer actually strips o↵\n",
    "additional characters from the end, so “include”, “includes”, “included”,\n",
    "and “including” are all replaced with “includ”.\n",
    "\n",
    "• Removal of non-words: Non-words and punctuation have been re-\n",
    "moved. All white spaces (tabs, newlines, spaces) have all been trimmed\n",
    "to a single space character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Vocabulary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getVocabList function will return a ndarray that contains vocab.txt\n",
    "def getVocabList():\n",
    "    data=np.loadtxt('vocab.txt',dtype='str')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#readFile function will return the contents of the file\n",
    "def readFile(filename):\n",
    "    f=open(filename)\n",
    "    contents=f.read()\n",
    "    f.close()\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processEmail function will divide the contents into single word and tranpose them to stem by\n",
    "#using porter stemmer algrithm,and then the function look up to the vocab-list to find wether\n",
    "#any word is in the vocab-list.If yes,put the index of the word into word_indices(List),then\n",
    "#return it\n",
    "def processEmail(email_contents):\n",
    "    '''将内容分割成单个的单词，然后用词干提取算法处理每一个单词得到它的词干，对照着词汇表(vocab-list)\n",
    "    查看每个词干是否存在于词汇表中，如果存在，则返回该单词在词汇表中对应的下标'''\n",
    "    #Load Vocabulary\n",
    "    vocabList=getVocabList()\n",
    "    \n",
    "    #init return value(it's a List)\n",
    "    word_indices=[]\n",
    "    \n",
    "    #Preprocess Email\n",
    "    \n",
    "    #Lower case(转换成全部小写)\n",
    "    email_contents=email_contents.lower()\n",
    "    #Strip all HTML\n",
    "    #Looks for any expression that starts with < and ends with > and replace\n",
    "    # and does not have any < or > in the tag it with a space\n",
    "    email_contents=re.sub('<[^<>]+>',' ',email_contents)\n",
    "    \n",
    "    #Handle Numbers\n",
    "    #Look for one or more characters between 0-9\n",
    "    email_contents=re.sub('[0-9]+','number',email_contents)\n",
    "    \n",
    "    #Handle URLS\n",
    "    #Look for strings starting with http:// or https://\n",
    "    email_contents=re.sub('(http|https)://[^\\s]*','httpaddr',email_contents)\n",
    "    \n",
    "    #Handle Email Address\n",
    "    #Look for strings with @ in the middle\n",
    "    email_contents=re.sub('[^\\s]+@[^\\s]+','emailaddr',email_contents)\n",
    "    \n",
    "    #Handle $ sign\n",
    "    email_contents=re.sub('[$]+','dollar',email_contents)\n",
    "    \n",
    "    #处理所有不是字母和数字的符号，将它们都换成空格\n",
    "    email_contents=re.sub('[\\W]+',' ',email_contents)\n",
    "    \n",
    "    #用re.split函数将其分割为单词\n",
    "    words=re.split(' ',email_contents)#words is a List\n",
    "    \n",
    "    #Tokenize Email\n",
    "    lenth=len(words)\n",
    "    for i in range(lenth):\n",
    "        if words[i]!=' ':\n",
    "            myporter=PorterStemmer()\n",
    "            words[i]=myporter.stem(words[i])\n",
    "    for i in range(lenth):\n",
    "        for j in range(1899):\n",
    "            if words[i]==vocabList[j][1]:\n",
    "                word_indices.append(vocabList[j][0])\n",
    "    return word_indices\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word indices:\n",
      "['86', '916', '794', '1077', '883', '370', '1699', '790', '1822', '1831', '883', '431', '1171', '794', '1002', '1893', '1364', '592', '1676', '238', '162', '89', '688', '945', '1663', '1120', '1062', '1699', '375', '1162', '479', '1893', '1510', '799', '1182', '1237', '810', '1895', '1440', '1547', '181', '1699', '1758', '1896', '688', '1676', '992', '961', '1477', '71', '530', '1699', '531']\n"
     ]
    }
   ],
   "source": [
    "#Extract Features\n",
    "file_contents=readFile('emailSample1.txt')\n",
    "word_indices=processEmail(file_contents)\n",
    "\n",
    "#Print Stats\n",
    "print('word indices:')\n",
    "print(word_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Extracting Features from Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emailFeatures(word_indices):\n",
    "    '''This function will return a featrue vector for the given email(word_indices)'''\n",
    "    n=1899\n",
    "    x=np.zeros((n,1))\n",
    "    lenth=len(word_indices)\n",
    "    for i in range(lenth):\n",
    "        x[int(word_indices[i])-1][0]=1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of feature vector: 1899 ,it should be 1899\n",
      "Number of non-zero entries: 45.0 ,it should be 45\n"
     ]
    }
   ],
   "source": [
    "#Feature Extraction\n",
    "#Extract Features\n",
    "file_contents=readFile('emailSample1.txt')\n",
    "word_indices=processEmail(file_contents)\n",
    "features=emailFeatures(word_indices)\n",
    "\n",
    "#Print Stats\n",
    "print('length of feature vector:',features.shape[0],',it should be 1899')\n",
    "print('Number of non-zero entries:',np.sum(features),',it should be 45')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training SVM for Spam Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 1899)\n"
     ]
    }
   ],
   "source": [
    "#Train Linear SVM for Spam Classification\n",
    "dataFile='spamTrain.mat'\n",
    "data=scio.loadmat(dataFile)\n",
    "X=data['X']\n",
    "y=data['y']\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhanghaoxu/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:99.825%\n",
      "It should be about 99.8%\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "C=0.1\n",
    "model=SVC(C=0.1,kernel='linear').fit(X,y)\n",
    "accuracy=model.score(X,y)\n",
    "print('Training Accuracy:'+str(accuracy*100)+'%')\n",
    "print('It should be about 99.8%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:98.9%\n",
      "It should be about 98.5%\n"
     ]
    }
   ],
   "source": [
    "#Test Spam Classification\n",
    "#load the test dataset\n",
    "dataFile='spamTest.mat'\n",
    "data=scio.loadmat(dataFile)\n",
    "Xtest=data['Xtest']\n",
    "ytest=data['ytest']\n",
    "accuracy_test=model.score(Xtest,ytest)\n",
    "print('Training Accuracy:'+str(accuracy_test*100)+'%')\n",
    "print('It should be about 98.5%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Top Predictors for Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00793208  0.01563324  0.05546492 ..., -0.08670606 -0.00661274\n",
      "   0.06506632]]\n",
      "our 0.500613736175\n",
      "click 0.465916390689\n",
      "remov 0.422869117061\n",
      "guarante 0.383621601794\n",
      "visit 0.367710398246\n",
      "basenumb 0.345064097946\n",
      "dollar 0.323632035796\n",
      "will 0.269724106037\n",
      "price 0.267297714618\n",
      "pleas 0.2611688867\n",
      "most 0.257298197952\n",
      "nbsp 0.25394145516\n",
      "lo 0.253466524314\n",
      "ga 0.248296990456\n",
      "hour 0.246404357832\n"
     ]
    }
   ],
   "source": [
    "print(model.coef_) #coef_ is an attributes of SVC objection,it is the weights(theta), it can \n",
    "#only be used when the method is 'linear'\n",
    "dict_weight={}\n",
    "#把每个权重以及对应的下标都加入字典\n",
    "for i in range(model.coef_.shape[1]):#note that shape of model.coef_ is 1*1899\n",
    "    dict_weight[model.coef_[0][i]]=i\n",
    "#对该字典以下列方式进行排序，返回的将是一个列表，列表中的每一个元素都是一个元祖，每个元祖包含两个元素：key值\n",
    "#和value值（排好序的）\n",
    "list_weight=sorted(dict_weight.items(),reverse=True)\n",
    "vocabList=getVocabList()\n",
    "#在vocabList中依次查找对应的下标，找出前十五个，表示就是分类为垃圾邮件的最重要的15个词干，因为它们被分配到的\n",
    "#权重最大\n",
    "for i in range(15):\n",
    "    print(vocabList[list_weight[i][1]][1],list_weight[i][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
